{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyORQJyy+jYCEkIXsO4marFm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Translator Tool\n","Made by: Wilfredo Aaron Sosa Ramos"],"metadata":{"id":"dIoN08x-1s31"}},{"cell_type":"markdown","source":["##1. Only text:"],"metadata":{"id":"oQI-boggS0A6"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"JTWopSkk1mqm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737134753551,"user_tz":300,"elapsed":9215,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"b640ea79-da14-43a0-9f2e-30105a355edd"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/2.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q langchain langchain_community langchain_core langchain_google_genai"]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"],"metadata":{"id":"xws_QUOZTLxo","executionInfo":{"status":"ok","timestamp":1737134898788,"user_tz":300,"elapsed":1459,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def split_text(text, chunk_size=1000):\n","    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]"],"metadata":{"id":"zp9CB4ctTv2q","executionInfo":{"status":"ok","timestamp":1737135152710,"user_tz":300,"elapsed":304,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["text = \"\"\"\n","Large Language Models: Revolutionizing Natural Language Processing\n","\n","In recent years, large language models (LLMs) have emerged as transformative tools in the field of artificial intelligence (AI), particularly in natural language processing (NLP). These models, such as OpenAI’s GPT-4, Google’s Bard, and others, have redefined how machines understand, interpret, and generate human language. With their unprecedented ability to produce coherent and contextually relevant text, LLMs are changing industries, research, and communication on a global scale. This essay delves into what LLMs are, their mechanisms, applications, challenges, and future prospects.\n","\n","Understanding LLMs\n","\n","Large language models are a class of AI systems trained on vast amounts of text data to perform a wide range of language-related tasks. They operate using deep learning, specifically transformer architectures introduced in the groundbreaking paper \"Attention Is All You Need\" by Vaswani et al. in 2017. Transformers leverage self-attention mechanisms to analyze the relationships between words in a sequence, enabling them to understand context more effectively than previous models like recurrent neural networks (RNNs) or long short-term memory (LSTM) networks.\n","\n","An LLM is characterized by its sheer size—measured in the number of parameters it uses. Parameters are the internal variables that a model adjusts during training to learn patterns in the data. For instance, GPT-3, an earlier but still significant model by OpenAI, has 175 billion parameters. The size and complexity of these models allow them to perform a vast array of language tasks, from summarization to translation and even creative writing.\n","\n","Training LLMs\n","\n","Training an LLM involves exposing it to enormous datasets containing text from books, websites, scientific articles, and more. During this process, the model learns patterns of grammar, syntax, and semantics, as well as facts about the world encoded in the training data. This enables it to generate human-like responses when given input.\n","\n","One crucial aspect of training is the balance between supervised and unsupervised learning. Most LLMs rely heavily on unsupervised pretraining, where they predict the next word in a sequence based on the preceding text. Fine-tuning, often done using smaller, domain-specific datasets, allows the model to specialize in certain tasks. However, this process requires immense computational resources, often involving clusters of high-performance GPUs or TPUs.\n","\n","Applications of LLMs\n","\n","LLMs have found applications across a wide range of industries. In healthcare, they assist with medical transcription, summarization of patient records, and even aiding in diagnosis by analyzing medical literature. In the legal sector, they streamline the review of contracts and legal documents, making the process faster and more accurate.\n","\n","Education has also benefited greatly from LLMs. Tools like AI-powered tutors and content generators can provide personalized learning experiences for students, adapt to their pace, and create tailored study materials. Similarly, in customer service, chatbots powered by LLMs handle inquiries with remarkable efficiency, reducing the need for human intervention in repetitive tasks.\n","\n","Creative industries have embraced LLMs for content generation, scriptwriting, and even music composition. They empower creators by acting as brainstorming partners or automating tedious tasks like generating first drafts. Additionally, in programming, LLMs like GitHub Copilot assist developers by suggesting code snippets, reducing the time needed to write complex algorithms.\n","\n","Ethical Considerations\n","\n","Despite their benefits, LLMs raise several ethical concerns. One of the primary challenges is bias. Since these models learn from data generated by humans, they can inadvertently inherit and propagate societal biases present in their training datasets. For example, gender, racial, or cultural biases may surface in the text they generate.\n","\n","Another concern is misinformation. LLMs can produce text that is coherent and convincing but factually incorrect. This has implications for spreading false information, especially when such models are used without proper safeguards. Addressing these issues requires rigorous evaluation, better training practices, and transparency in how models are built and deployed.\n","\n","Privacy is another area of concern. LLMs trained on publicly available data might inadvertently include sensitive or personally identifiable information (PII) in their output. Techniques like differential privacy, which ensures that individual data points cannot be traced back to the original dataset, are being explored to mitigate this risk.\n","\n","Technical Limitations\n","\n","While LLMs are impressive, they are not without limitations. Their reliance on large-scale training data means they may struggle with tasks that require reasoning beyond the patterns they have learned. For instance, complex mathematical proofs or solving abstract logical puzzles are challenging for these models.\n","\n","Moreover, the energy consumption associated with training and deploying LLMs is a growing concern. Training a single large model can have a significant carbon footprint, prompting researchers to explore more energy-efficient architectures and training methods.\n","\n","Future Directions\n","\n","The future of LLMs holds exciting possibilities. Researchers are working on creating smaller, more efficient models that achieve comparable performance to their larger counterparts. This is crucial for democratizing access to AI, as smaller models can run on less powerful hardware, making them accessible to a broader range of users.\n","\n","Another area of focus is improving the interpretability of LLMs. Understanding why a model generates a particular response can help build trust and enable more effective debugging. Techniques like attention visualization and feature attribution are being developed to shed light on the inner workings of these models.\n","\n","Collaboration between academia, industry, and policymakers will be essential to address the ethical and societal challenges posed by LLMs. Establishing guidelines for responsible AI usage and ensuring equitable access to these technologies will be key to maximizing their benefits while minimizing potential harms.\n","\n","Conclusion\n","\n","Large language models represent a significant milestone in the evolution of artificial intelligence. Their ability to understand and generate human language with remarkable accuracy has opened up new possibilities in countless fields. However, as with any powerful technology, their use comes with responsibilities. By addressing ethical concerns, technical limitations, and environmental impacts, we can ensure that LLMs are developed and deployed for the greater good. As research continues, the potential of these models to transform how we interact with technology, information, and each other remains boundless.\n","\"\"\""],"metadata":{"id":"dt4IbkbrUt3T","executionInfo":{"status":"ok","timestamp":1737135392721,"user_tz":300,"elapsed":382,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["splited_text = split_text(text)"],"metadata":{"id":"BoHBn05oVp8t","executionInfo":{"status":"ok","timestamp":1737135429516,"user_tz":300,"elapsed":248,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["splited_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24yGJO0dVyB9","executionInfo":{"status":"ok","timestamp":1737135433505,"user_tz":300,"elapsed":290,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"81252523-4b8f-4370-8de6-45a9df5ae780"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['\\nLarge Language Models: Revolutionizing Natural Language Processing\\n\\nIn recent years, large language models (LLMs) have emerged as transformative tools in the field of artificial intelligence (AI), particularly in natural language processing (NLP). These models, such as OpenAI’s GPT-4, Google’s Bard, and others, have redefined how machines understand, interpret, and generate human language. With their unprecedented ability to produce coherent and contextually relevant text, LLMs are changing industries, research, and communication on a global scale. This essay delves into what LLMs are, their mechanisms, applications, challenges, and future prospects.\\n\\nUnderstanding LLMs\\n\\nLarge language models are a class of AI systems trained on vast amounts of text data to perform a wide range of language-related tasks. They operate using deep learning, specifically transformer architectures introduced in the groundbreaking paper \"Attention Is All You Need\" by Vaswani et al. in 2017. Transformers lev',\n"," 'erage self-attention mechanisms to analyze the relationships between words in a sequence, enabling them to understand context more effectively than previous models like recurrent neural networks (RNNs) or long short-term memory (LSTM) networks.\\n\\nAn LLM is characterized by its sheer size—measured in the number of parameters it uses. Parameters are the internal variables that a model adjusts during training to learn patterns in the data. For instance, GPT-3, an earlier but still significant model by OpenAI, has 175 billion parameters. The size and complexity of these models allow them to perform a vast array of language tasks, from summarization to translation and even creative writing.\\n\\nTraining LLMs\\n\\nTraining an LLM involves exposing it to enormous datasets containing text from books, websites, scientific articles, and more. During this process, the model learns patterns of grammar, syntax, and semantics, as well as facts about the world encoded in the training data. This enables it to',\n"," ' generate human-like responses when given input.\\n\\nOne crucial aspect of training is the balance between supervised and unsupervised learning. Most LLMs rely heavily on unsupervised pretraining, where they predict the next word in a sequence based on the preceding text. Fine-tuning, often done using smaller, domain-specific datasets, allows the model to specialize in certain tasks. However, this process requires immense computational resources, often involving clusters of high-performance GPUs or TPUs.\\n\\nApplications of LLMs\\n\\nLLMs have found applications across a wide range of industries. In healthcare, they assist with medical transcription, summarization of patient records, and even aiding in diagnosis by analyzing medical literature. In the legal sector, they streamline the review of contracts and legal documents, making the process faster and more accurate.\\n\\nEducation has also benefited greatly from LLMs. Tools like AI-powered tutors and content generators can provide personalized le',\n"," 'arning experiences for students, adapt to their pace, and create tailored study materials. Similarly, in customer service, chatbots powered by LLMs handle inquiries with remarkable efficiency, reducing the need for human intervention in repetitive tasks.\\n\\nCreative industries have embraced LLMs for content generation, scriptwriting, and even music composition. They empower creators by acting as brainstorming partners or automating tedious tasks like generating first drafts. Additionally, in programming, LLMs like GitHub Copilot assist developers by suggesting code snippets, reducing the time needed to write complex algorithms.\\n\\nEthical Considerations\\n\\nDespite their benefits, LLMs raise several ethical concerns. One of the primary challenges is bias. Since these models learn from data generated by humans, they can inadvertently inherit and propagate societal biases present in their training datasets. For example, gender, racial, or cultural biases may surface in the text they generate.\\n\\n',\n"," 'Another concern is misinformation. LLMs can produce text that is coherent and convincing but factually incorrect. This has implications for spreading false information, especially when such models are used without proper safeguards. Addressing these issues requires rigorous evaluation, better training practices, and transparency in how models are built and deployed.\\n\\nPrivacy is another area of concern. LLMs trained on publicly available data might inadvertently include sensitive or personally identifiable information (PII) in their output. Techniques like differential privacy, which ensures that individual data points cannot be traced back to the original dataset, are being explored to mitigate this risk.\\n\\nTechnical Limitations\\n\\nWhile LLMs are impressive, they are not without limitations. Their reliance on large-scale training data means they may struggle with tasks that require reasoning beyond the patterns they have learned. For instance, complex mathematical proofs or solving abstra',\n"," 'ct logical puzzles are challenging for these models.\\n\\nMoreover, the energy consumption associated with training and deploying LLMs is a growing concern. Training a single large model can have a significant carbon footprint, prompting researchers to explore more energy-efficient architectures and training methods.\\n\\nFuture Directions\\n\\nThe future of LLMs holds exciting possibilities. Researchers are working on creating smaller, more efficient models that achieve comparable performance to their larger counterparts. This is crucial for democratizing access to AI, as smaller models can run on less powerful hardware, making them accessible to a broader range of users.\\n\\nAnother area of focus is improving the interpretability of LLMs. Understanding why a model generates a particular response can help build trust and enable more effective debugging. Techniques like attention visualization and feature attribution are being developed to shed light on the inner workings of these models.\\n\\nCollaborat',\n"," 'ion between academia, industry, and policymakers will be essential to address the ethical and societal challenges posed by LLMs. Establishing guidelines for responsible AI usage and ensuring equitable access to these technologies will be key to maximizing their benefits while minimizing potential harms.\\n\\nConclusion\\n\\nLarge language models represent a significant milestone in the evolution of artificial intelligence. Their ability to understand and generate human language with remarkable accuracy has opened up new possibilities in countless fields. However, as with any powerful technology, their use comes with responsibilities. By addressing ethical concerns, technical limitations, and environmental impacts, we can ensure that LLMs are developed and deployed for the greater good. As research continues, the potential of these models to transform how we interact with technology, information, and each other remains boundless.\\n']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["len(splited_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waMMTkxgVzVi","executionInfo":{"status":"ok","timestamp":1737135441549,"user_tz":300,"elapsed":281,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"1b82df6f-fb8b-49dc-90ca-9224a43733fc"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.schema import (\n","       AIMessage,\n","       HumanMessage,\n","       SystemMessage\n","  )\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0.7)"],"metadata":{"id":"Vat7EfmKWPxT","executionInfo":{"status":"ok","timestamp":1737135719521,"user_tz":300,"elapsed":315,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def translate_text(text, target_lang):\n","    messages = [\n","        SystemMessage(content=f\"You are a professional translator specializing in creating fluent and accurate translations into {target_lang}, ensuring cultural and linguistic nuances are respected.\"),\n","        HumanMessage(content=f\"\"\"Translate the following text into {target_lang}, ensuring that the translation maintains the original meaning, tone, and context:\n","\n","        Text:\n","        {text}\n","\n","        Ensure the translation is fluent, accurate, and culturally appropriate for readers in {target_lang}. Just give me the translated text.\n","        \"\"\")\n","    ]\n","\n","    result = llm.invoke(messages)\n","\n","    return result.content"],"metadata":{"id":"T8uY8azgV3E2","executionInfo":{"status":"ok","timestamp":1737136321237,"user_tz":300,"elapsed":305,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["translate_text(splited_text[0], \"es\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"ZmAx7dJTXy_K","executionInfo":{"status":"ok","timestamp":1737136324550,"user_tz":300,"elapsed":13,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"b06fa132-b3e6-4200-d6cf-2d516d324735"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Modelos de Lenguaje Extensos: Revolucionando el Procesamiento del Lenguaje Natural\\n\\nEn los últimos años, los modelos de lenguaje extensos (MLE) han surgido como herramientas transformadoras en el campo de la inteligencia artificial (IA), particularmente en el procesamiento del lenguaje natural (PLN). Estos modelos, como GPT-4 de OpenAI, Bard de Google, y otros, han redefinido la manera en que las máquinas entienden, interpretan y generan el lenguaje humano. Con su capacidad sin precedentes para producir texto coherente y contextualmente relevante, los MLE están cambiando industrias, la investigación y la comunicación a escala global. Este ensayo profundiza en qué son los MLE, sus mecanismos, aplicaciones, desafíos y perspectivas futuras.\\n\\nEntendiendo los MLE\\n\\nLos modelos de lenguaje extensos son una clase de sistemas de IA entrenados con vastas cantidades de datos de texto para realizar una amplia gama de tareas relacionadas con el lenguaje. Operan utilizando aprendizaje profundo, específicamente arquitecturas de transformadores introducidas en el innovador artículo \"Attention Is All You Need\" de Vaswani et al. en 2017. Los transformadores lev'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["import threading\n","\n","def translate_texts_multithreaded_and_join(splitted_texts, target_lang):\n","    results = [None] * len(splitted_texts)\n","    threads = []\n","\n","    # Worker function for each thread\n","    def translate_part(index, text):\n","        results[index] = translate_text(text, target_lang)\n","\n","    # Create and start threads\n","    for i, text in enumerate(splitted_texts):\n","        thread = threading.Thread(target=translate_part, args=(i, text))\n","        threads.append(thread)\n","        thread.start()\n","\n","    # Wait for all threads to complete\n","    for thread in threads:\n","        thread.join()\n","\n","    # Join the translated results in order\n","    translated_text = \"\".join(results)\n","    return translated_text"],"metadata":{"id":"BfcO0rVyafTc","executionInfo":{"status":"ok","timestamp":1737136670641,"user_tz":300,"elapsed":309,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["final_result = translate_texts_multithreaded_and_join(splited_text, \"es\")"],"metadata":{"id":"bqF0gjwKaiZa","executionInfo":{"status":"ok","timestamp":1737136737193,"user_tz":300,"elapsed":2458,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["final_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817},"id":"DMMw3wNsaxhB","executionInfo":{"status":"ok","timestamp":1737136742530,"user_tz":300,"elapsed":11,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"ad8252db-e54f-46d1-c1e7-9e31b5c3c0fd"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Modelos de Lenguaje Extensos: Revolucionando el Procesamiento del Lenguaje Natural\\n\\nEn los últimos años, los modelos de lenguaje extensos (MLE) han surgido como herramientas transformadoras en el campo de la inteligencia artificial (IA), particularmente en el procesamiento del lenguaje natural (PLN). Estos modelos, como GPT-4 de OpenAI, Bard de Google y otros, han redefinido cómo las máquinas comprenden, interpretan y generan el lenguaje humano. Con su capacidad sin precedentes para producir texto coherente y contextualmente relevante, los MLE están cambiando industrias, la investigación y la comunicación a escala global. Este ensayo profundiza en qué son los MLE, sus mecanismos, aplicaciones, desafíos y perspectivas futuras.\\n\\nComprendiendo los MLE\\n\\nLos modelos de lenguaje extensos son una clase de sistemas de IA entrenados con grandes cantidades de datos de texto para realizar una amplia gama de tareas relacionadas con el lenguaje. Operan utilizando aprendizaje profundo, específicamente arquitecturas de transformadores introducidas en el innovador artículo \"Attention Is All You Need\" de Vaswani et al. en 2017. Los transformadores levLos LLM aprovechan mecanismos de autoatención para analizar las relaciones entre las palabras en una secuencia, lo que les permite comprender el contexto de manera más efectiva que modelos anteriores como las redes neuronales recurrentes (RNN) o las redes de memoria a corto plazo (LSTM).\\n\\nUn LLM se caracteriza por su gran tamaño, medido por la cantidad de parámetros que utiliza. Los parámetros son las variables internas que un modelo ajusta durante el entrenamiento para aprender patrones en los datos. Por ejemplo, GPT-3, un modelo anterior pero aún significativo de OpenAI, tiene 175 mil millones de parámetros. El tamaño y la complejidad de estos modelos les permiten realizar una amplia gama de tareas de lenguaje, desde la creación de resúmenes hasta la traducción e incluso la escritura creativa.\\n\\nEntrenamiento de LLM\\n\\nEl entrenamiento de un LLM implica exponerlo a conjuntos de datos enormes que contienen texto de libros, sitios web, artículos científicos y más. Durante este proceso, el modelo aprende patrones de gramática, sintaxis y semántica, así como hechos sobre el mundo codificados en los datos de entrenamiento. Esto le permitegenerar respuestas similares a las humanas cuando se les proporciona información.\\n\\nUn aspecto crucial del entrenamiento es el equilibrio entre el aprendizaje supervisado y no supervisado. La mayoría de los LLM se basan en gran medida en el preentrenamiento no supervisado, donde predicen la siguiente palabra en una secuencia basándose en el texto precedente. El ajuste fino, que a menudo se realiza utilizando conjuntos de datos más pequeños y específicos de un dominio, permite que el modelo se especialice en ciertas tareas. Sin embargo, este proceso requiere inmensos recursos computacionales, a menudo involucrando clústeres de GPU o TPU de alto rendimiento.\\n\\nAplicaciones de los LLM\\n\\nLos LLM han encontrado aplicaciones en una amplia gama de industrias. En el sector de la salud, ayudan con la transcripción médica, el resumen de historiales de pacientes e incluso contribuyen al diagnóstico mediante el análisis de la literatura médica. En el sector legal, agilizan la revisión de contratos y documentos legales, haciendo que el proceso sea más rápido y preciso.\\n\\nLa educación también se ha beneficiado enormemente de los LLM. Herramientas como los tutores impulsados por IA y los generadores de contenido pueden proporcionar un aprendizaje personalizado.Las experiencias de aprendizaje para los estudiantes, se adaptan a su ritmo y crean materiales de estudio personalizados. De manera similar, en el servicio al cliente, los chatbots impulsados por LLM gestionan las consultas con notable eficiencia, reduciendo la necesidad de intervención humana en tareas repetitivas.\\n\\nLas industrias creativas han adoptado los LLM para la generación de contenido, la escritura de guiones e incluso la composición musical. Empoderan a los creadores actuando como socios de lluvia de ideas o automatizando tareas tediosas como la generación de primeros borradores. Además, en la programación, los LLM como GitHub Copilot ayudan a los desarrolladores sugiriendo fragmentos de código, reduciendo el tiempo necesario para escribir algoritmos complejos.\\n\\nConsideraciones Éticas\\n\\nA pesar de sus beneficios, los LLM plantean varias preocupaciones éticas. Uno de los principales desafíos es el sesgo. Dado que estos modelos aprenden de datos generados por humanos, pueden, sin querer, heredar y propagar los sesgos sociales presentes en sus conjuntos de datos de entrenamiento. Por ejemplo, pueden surgir sesgos de género, raciales o culturales en el texto que generan.Otra preocupación es la desinformación. Los LLM pueden generar texto que es coherente y convincente pero, a la vez, incorrecto en cuanto a los hechos. Esto tiene implicaciones en la difusión de información falsa, especialmente cuando estos modelos se utilizan sin las debidas precauciones. Para abordar estos problemas, se requiere una evaluación rigurosa, mejores prácticas de entrenamiento y transparencia en cómo se construyen e implementan los modelos.\\n\\nLa privacidad es otra área de preocupación. Los LLM entrenados con datos disponibles públicamente podrían incluir inadvertidamente información sensible o de identificación personal (PII) en su salida. Se están explorando técnicas como la privacidad diferencial, que garantiza que los puntos de datos individuales no puedan rastrearse hasta el conjunto de datos original, para mitigar este riesgo.\\n\\nLimitaciones técnicas\\n\\nSi bien los LLM son impresionantes, no están exentos de limitaciones. Su dependencia de datos de entrenamiento a gran escala significa que pueden tener dificultades con tareas que requieren un razonamiento más allá de los patrones que han aprendido. Por ejemplo, demostraciones matemáticas complejas o la resolución de problemas abstractos.Los acertijos lógicos son un desafío para estos modelos.\\n\\nAdemás, el consumo de energía asociado con el entrenamiento y la implementación de LLM es una preocupación creciente. El entrenamiento de un solo modelo grande puede tener una huella de carbono significativa, lo que impulsa a los investigadores a explorar arquitecturas y métodos de entrenamiento más eficientes en cuanto a energía.\\n\\nDirecciones Futuras\\n\\nEl futuro de los LLM presenta posibilidades emocionantes. Los investigadores están trabajando en la creación de modelos más pequeños y eficientes que alcancen un rendimiento comparable al de sus homólogos más grandes. Esto es crucial para democratizar el acceso a la IA, ya que los modelos más pequeños pueden ejecutarse en hardware menos potente, lo que los hace accesibles a una gama más amplia de usuarios.\\n\\nOtra área de enfoque es mejorar la interpretabilidad de los LLM. Comprender por qué un modelo genera una respuesta particular puede ayudar a generar confianza y permitir una depuración más eficaz. Se están desarrollando técnicas como la visualización de la atención y la atribución de características para arrojar luz sobre el funcionamiento interno de estos modelos.\\n\\nColaboraLa colaboración entre el mundo académico, la industria y los responsables políticos será esencial para abordar los desafíos éticos y sociales que plantean los LLM. Establecer directrices para un uso responsable de la IA y garantizar un acceso equitativo a estas tecnologías será clave para maximizar sus beneficios y minimizar los posibles daños.\\n\\nConclusión\\n\\nLos modelos de lenguaje grandes representan un hito significativo en la evolución de la inteligencia artificial. Su capacidad para comprender y generar lenguaje humano con notable precisión ha abierto nuevas posibilidades en innumerables campos. Sin embargo, como ocurre con cualquier tecnología poderosa, su uso conlleva responsabilidades. Al abordar las preocupaciones éticas, las limitaciones técnicas y los impactos ambientales, podemos asegurar que los LLM se desarrollen e implementen para el bien común. A medida que la investigación continúa, el potencial de estos modelos para transformar la forma en que interactuamos con la tecnología, la información y entre nosotros sigue siendo ilimitado.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["##2. PDF, TXT or DOCX files"],"metadata":{"id":"PW75t7uga5RE"}},{"cell_type":"code","source":["!pip install -q pypdf docx2txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNbQkyyza7g8","executionInfo":{"status":"ok","timestamp":1737136821180,"user_tz":300,"elapsed":5244,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"7e309d4d-bf7e-40a7-f567-9e27e4717c67"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/298.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import tempfile\n","import uuid\n","import requests\n","\n","class FileHandler:\n","    def __init__(self, file_loader, file_extension):\n","        self.file_loader = file_loader\n","        self.file_extension = file_extension\n","\n","    def load(self, url):\n","        # Generate a unique filename with a UUID prefix\n","        unique_filename = f\"{uuid.uuid4()}.{self.file_extension}\"\n","\n","        try:\n","            # Download the file from the URL and save it to a temporary file\n","            response = requests.get(url, timeout=10)\n","            response.raise_for_status()  # Raise an HTTPError for bad responses\n","\n","            with tempfile.NamedTemporaryFile(delete=False, prefix=unique_filename) as temp_file:\n","                temp_file.write(response.content)\n","                temp_file_path = temp_file.name\n","\n","        except requests.exceptions.RequestException as req_err:\n","            raise Exception(f\"Failed to download file from URL\", url) from req_err\n","        except Exception as e:\n","            raise Exception(f\"Failed to handle file download\", url) from e\n","\n","        # Use the file_loader to load the documents\n","        try:\n","            loader = self.file_loader(file_path=temp_file_path)\n","        except Exception as e:\n","            raise Exception(f\"No file found\", temp_file_path) from e\n","\n","        try:\n","            documents = loader.load()\n","        except Exception as e:\n","            raise Exception(f\"No file content available\", temp_file_path) from e\n","\n","        # Remove the temporary file\n","        os.remove(temp_file_path)\n","\n","        return documents"],"metadata":{"id":"nwVrXBuLcTm4","executionInfo":{"status":"ok","timestamp":1737137176715,"user_tz":300,"elapsed":296,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","splitter = RecursiveCharacterTextSplitter(\n","    chunk_size = 1000,\n","    chunk_overlap = 0\n",")\n","\n","def load_pdf_documents(pdf_url: str, verbose=True):\n","    pdf_loader = FileHandler(PyPDFLoader, \"pdf\")\n","    docs = pdf_loader.load(pdf_url)\n","\n","    if docs:\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found PDF file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_txt_documents(notes_url: str, verbose=True):\n","    notes_loader = FileHandler(TextLoader, \"txt\")\n","    docs = notes_loader.load(notes_url)\n","\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found TXT file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs\n","\n","def load_docx_documents(docx_url: str, verbose=True):\n","    docx_handler = FileHandler(Docx2txtLoader, 'docx')\n","    docs = docx_handler.load(docx_url)\n","    if docs:\n","\n","        split_docs = splitter.split_documents(docs)\n","\n","        if verbose:\n","            print(f\"Found DOCX file\")\n","            print(f\"Splitting documents into {len(split_docs)} chunks\")\n","\n","        return split_docs"],"metadata":{"id":"2_Gq6st6bENr","executionInfo":{"status":"ok","timestamp":1737137505417,"user_tz":300,"elapsed":251,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["pdf_docs = load_pdf_documents(\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/llms.pdf\")\n","txt_docs = load_txt_documents(\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/llms.txt\")\n","docx_docs = load_docx_documents(\"https://raw.github.com/AaronSosaRamos/mission-flights/main/files-for-test/llms.docx\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jo5yiqYNduAw","executionInfo":{"status":"ok","timestamp":1737138110402,"user_tz":300,"elapsed":1538,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"14a33486-2bd8-4526-cb68-701a11d4e69c"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Found PDF file\n","Splitting documents into 3 chunks\n","Found TXT file\n","Splitting documents into 3 chunks\n","Found DOCX file\n","Splitting documents into 3 chunks\n"]}]},{"cell_type":"code","source":["pdf_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OpZikUgEgAcV","executionInfo":{"status":"ok","timestamp":1737138116868,"user_tz":300,"elapsed":252,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"a3cc4eff-b377-440d-e96d-98b921b757ef"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/tmp/82a48b5b-15bb-4d44-b846-239cb9d884ca.pdfnvdsms0f', 'page': 0}, page_content=\"LLMs and Transformers \\nIntroduction \\nLarge Language Models (LLMs) and Transformers have revolutionized the field of artificial \\nintelligence, specifically in natural language processing (NLP). These models are designed to \\nunderstand, generate, and manipulate human language with remarkable accuracy, enabling \\na wide range of applications. \\nTransformers \\nThe Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' \\nby Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a \\nmechanism called self-attention, allowing the model to weigh the importance of different \\nwords in a sentence relative to each other, regardless of their distance. This capability \\nenables Transformers to capture long-range dependencies in text more effectively than \\nprevious models. \\nKey Components of Transformers \\n- **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The\"),\n"," Document(metadata={'source': '/tmp/82a48b5b-15bb-4d44-b846-239cb9d884ca.pdfnvdsms0f', 'page': 0}, page_content=\"encoder processes the input sequence, while the decoder generates the output sequence. \\n- **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of \\nthe input while processing. \\n- **Positional Encoding**: Since Transformers lack inherent sequential information, \\npositional encodings are added to input embeddings. \\nLarge Language Models (LLMs) \\nLLMs are built upon the Transformer architecture and trained on massive datasets to \\nperform a variety of language tasks. Examples of such models include OpenAI's GPT series, \\nGoogle's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in \\ntasks like text generation, summarization, translation, and question-answering. \\nApplications of LLMs \\n- **Text Generation**: Generating coherent and contextually appropriate text. \\n- **Machine Translation**: Translating text between languages with high accuracy. \\n- **Summarization**: Condensing long pieces of text into concise summaries.\"),\n"," Document(metadata={'source': '/tmp/82a48b5b-15bb-4d44-b846-239cb9d884ca.pdfnvdsms0f', 'page': 0}, page_content='- **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT. \\nChallenges and Future Directions \\nDespite their success, LLMs and Transformers face challenges such as high computational \\ncosts, the risk of generating biased or incorrect information, and difficulties in interpreting \\ntheir decision-making processes. Researchers are exploring approaches to address these \\nissues and enhance the efficiency and fairness of these models.')]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["txt_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGjz5L6YgC66","executionInfo":{"status":"ok","timestamp":1737138123841,"user_tz":300,"elapsed":263,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"b9c393dc-e9c1-4975-adee-94be2de50293"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/tmp/1721b595-68c2-41a6-b24c-6c0c051dc941.txtre82xr38'}, page_content=\"LLMs and Transformers \\nIntroduction \\nLarge Language Models (LLMs) and Transformers have revolutionized the field of artificial \\nintelligence, specifically in natural language processing (NLP). These models are designed to \\nunderstand, generate, and manipulate human language with remarkable accuracy, enabling \\na wide range of applications. \\nTransformers \\nThe Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' \\nby Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a \\nmechanism called self-attention, allowing the model to weigh the importance of different \\nwords in a sentence relative to each other, regardless of their distance. This capability \\nenables Transformers to capture long-range dependencies in text more effectively than \\nprevious models. \\nKey Components of Transformers - **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The\"),\n"," Document(metadata={'source': '/tmp/1721b595-68c2-41a6-b24c-6c0c051dc941.txtre82xr38'}, page_content=\"encoder processes the input sequence, while the decoder generates the output sequence. - **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of \\nthe input while processing. - **Positional Encoding**: Since Transformers lack inherent sequential information, \\npositional encodings are added to input embeddings. \\nLarge Language Models (LLMs) \\nLLMs are built upon the Transformer architecture and trained on massive datasets to \\nperform a variety of language tasks. Examples of such models include OpenAI's GPT series, \\nGoogle's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in \\ntasks like text generation, summarization, translation, and question-answering.\"),\n"," Document(metadata={'source': '/tmp/1721b595-68c2-41a6-b24c-6c0c051dc941.txtre82xr38'}, page_content='Applications of LLMs - **Text Generation**: Generating coherent and contextually appropriate text. - **Machine Translation**: Translating text between languages with high accuracy. - **Summarization**: Condensing long pieces of text into concise summaries. - **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT. \\nChallenges and Future Directions \\nDespite their success, LLMs and Transformers face challenges such as high computational \\ncosts, the risk of generating biased or incorrect information, and difficulties in interpreting \\ntheir decision-making processes. Researchers are exploring approaches to address these \\nissues and enhance the efficiency and fairness of these models.')]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["docx_docs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wuKP0P3KgElW","executionInfo":{"status":"ok","timestamp":1737138131122,"user_tz":300,"elapsed":350,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"17ea7a21-a453-42de-bcdf-e8899fff601a"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={'source': '/tmp/5b49e767-7d5d-494e-bfde-1370d89e2b42.docxjsq1jiwc'}, page_content=\"LLMs and Transformers\\n\\nIntroduction\\n\\nLarge Language Models (LLMs) and Transformers have revolutionized the field of artificial intelligence, specifically in natural language processing (NLP). These models are designed to understand, generate, and manipulate human language with remarkable accuracy, enabling a wide range of applications.\\n\\nTransformers\\n\\nThe Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' by Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a mechanism called self-attention, allowing the model to weigh the importance of different words in a sentence relative to each other, regardless of their distance. This capability enables Transformers to capture long-range dependencies in text more effectively than previous models.\\n\\nKey Components of Transformers\"),\n"," Document(metadata={'source': '/tmp/5b49e767-7d5d-494e-bfde-1370d89e2b42.docxjsq1jiwc'}, page_content=\"- **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence.\\n- **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of the input while processing.\\n- **Positional Encoding**: Since Transformers lack inherent sequential information, positional encodings are added to input embeddings.\\n\\nLarge Language Models (LLMs)\\n\\nLLMs are built upon the Transformer architecture and trained on massive datasets to perform a variety of language tasks. Examples of such models include OpenAI's GPT series, Google's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in tasks like text generation, summarization, translation, and question-answering.\\n\\nApplications of LLMs\"),\n"," Document(metadata={'source': '/tmp/5b49e767-7d5d-494e-bfde-1370d89e2b42.docxjsq1jiwc'}, page_content='- **Text Generation**: Generating coherent and contextually appropriate text.\\n- **Machine Translation**: Translating text between languages with high accuracy.\\n- **Summarization**: Condensing long pieces of text into concise summaries.\\n- **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT.\\n\\nChallenges and Future Directions\\n\\nDespite their success, LLMs and Transformers face challenges such as high computational costs, the risk of generating biased or incorrect information, and difficulties in interpreting their decision-making processes. Researchers are exploring approaches to address these issues and enhance the efficiency and fairness of these models.')]"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["from typing import List\n","from langchain.schema import Document\n","\n","def join_docs_content(docs: List[Document], separator: str = \"\\n\\n\") -> str:\n","    \"\"\"Joins the content of a list of LangChain Documents.\n","\n","    Args:\n","        docs: A list of LangChain Document objects.\n","        separator: The separator to use between document contents.\n","                   Defaults to two newlines.\n","\n","    Returns:\n","        A string containing the joined content of all documents.\n","    \"\"\"\n","    return separator.join([doc.page_content for doc in docs])"],"metadata":{"id":"CXsvr0IqgFlW","executionInfo":{"status":"ok","timestamp":1737143878657,"user_tz":300,"elapsed":910,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["pdf_full_text = join_docs_content(pdf_docs)\n","txt_full_text = join_docs_content(txt_docs)\n","docx_full_text = join_docs_content(docx_docs)"],"metadata":{"id":"SMjXuKwM2AcY","executionInfo":{"status":"ok","timestamp":1737143894468,"user_tz":300,"elapsed":467,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["print(f\"PDF Full Text: {pdf_full_text}\")\n","print(\"=======================================================================\")\n","print(f\"TXT Full Text: {txt_full_text}\")\n","print(\"=======================================================================\")\n","print(f\"DOCX Full Text: {docx_full_text}\")\n","print(\"=======================================================================\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IlEcALD2EhR","executionInfo":{"status":"ok","timestamp":1737143957092,"user_tz":300,"elapsed":310,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"70909168-e5db-4854-c62f-34d35d11a17d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["PDF Full Text: LLMs and Transformers \n","Introduction \n","Large Language Models (LLMs) and Transformers have revolutionized the field of artificial \n","intelligence, specifically in natural language processing (NLP). These models are designed to \n","understand, generate, and manipulate human language with remarkable accuracy, enabling \n","a wide range of applications. \n","Transformers \n","The Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' \n","by Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a \n","mechanism called self-attention, allowing the model to weigh the importance of different \n","words in a sentence relative to each other, regardless of their distance. This capability \n","enables Transformers to capture long-range dependencies in text more effectively than \n","previous models. \n","Key Components of Transformers \n","- **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The\n","\n","encoder processes the input sequence, while the decoder generates the output sequence. \n","- **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of \n","the input while processing. \n","- **Positional Encoding**: Since Transformers lack inherent sequential information, \n","positional encodings are added to input embeddings. \n","Large Language Models (LLMs) \n","LLMs are built upon the Transformer architecture and trained on massive datasets to \n","perform a variety of language tasks. Examples of such models include OpenAI's GPT series, \n","Google's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in \n","tasks like text generation, summarization, translation, and question-answering. \n","Applications of LLMs \n","- **Text Generation**: Generating coherent and contextually appropriate text. \n","- **Machine Translation**: Translating text between languages with high accuracy. \n","- **Summarization**: Condensing long pieces of text into concise summaries.\n","\n","- **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT. \n","Challenges and Future Directions \n","Despite their success, LLMs and Transformers face challenges such as high computational \n","costs, the risk of generating biased or incorrect information, and difficulties in interpreting \n","their decision-making processes. Researchers are exploring approaches to address these \n","issues and enhance the efficiency and fairness of these models.\n","=======================================================================\n","TXT Full Text: LLMs and Transformers \n","Introduction \n","Large Language Models (LLMs) and Transformers have revolutionized the field of artificial \n","intelligence, specifically in natural language processing (NLP). These models are designed to \n","understand, generate, and manipulate human language with remarkable accuracy, enabling \n","a wide range of applications. \n","Transformers \n","The Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' \n","by Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a \n","mechanism called self-attention, allowing the model to weigh the importance of different \n","words in a sentence relative to each other, regardless of their distance. This capability \n","enables Transformers to capture long-range dependencies in text more effectively than \n","previous models. \n","Key Components of Transformers - **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The\n","\n","encoder processes the input sequence, while the decoder generates the output sequence. - **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of \n","the input while processing. - **Positional Encoding**: Since Transformers lack inherent sequential information, \n","positional encodings are added to input embeddings. \n","Large Language Models (LLMs) \n","LLMs are built upon the Transformer architecture and trained on massive datasets to \n","perform a variety of language tasks. Examples of such models include OpenAI's GPT series, \n","Google's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in \n","tasks like text generation, summarization, translation, and question-answering.\n","\n","Applications of LLMs - **Text Generation**: Generating coherent and contextually appropriate text. - **Machine Translation**: Translating text between languages with high accuracy. - **Summarization**: Condensing long pieces of text into concise summaries. - **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT. \n","Challenges and Future Directions \n","Despite their success, LLMs and Transformers face challenges such as high computational \n","costs, the risk of generating biased or incorrect information, and difficulties in interpreting \n","their decision-making processes. Researchers are exploring approaches to address these \n","issues and enhance the efficiency and fairness of these models.\n","=======================================================================\n","DOCX Full Text: LLMs and Transformers\n","\n","Introduction\n","\n","Large Language Models (LLMs) and Transformers have revolutionized the field of artificial intelligence, specifically in natural language processing (NLP). These models are designed to understand, generate, and manipulate human language with remarkable accuracy, enabling a wide range of applications.\n","\n","Transformers\n","\n","The Transformer architecture, introduced in the seminal paper 'Attention Is All You Need' by Vaswani et al. in 2017, is the backbone of many modern NLP systems. It leverages a mechanism called self-attention, allowing the model to weigh the importance of different words in a sentence relative to each other, regardless of their distance. This capability enables Transformers to capture long-range dependencies in text more effectively than previous models.\n","\n","Key Components of Transformers\n","\n","- **Encoder and Decoder**: The Transformer consists of an encoder and a decoder. The encoder processes the input sequence, while the decoder generates the output sequence.\n","- **Self-Attention Mechanism**: This mechanism helps the model focus on relevant parts of the input while processing.\n","- **Positional Encoding**: Since Transformers lack inherent sequential information, positional encodings are added to input embeddings.\n","\n","Large Language Models (LLMs)\n","\n","LLMs are built upon the Transformer architecture and trained on massive datasets to perform a variety of language tasks. Examples of such models include OpenAI's GPT series, Google's BERT, and Meta's LLaMA. These models achieve state-of-the-art performance in tasks like text generation, summarization, translation, and question-answering.\n","\n","Applications of LLMs\n","\n","- **Text Generation**: Generating coherent and contextually appropriate text.\n","- **Machine Translation**: Translating text between languages with high accuracy.\n","- **Summarization**: Condensing long pieces of text into concise summaries.\n","- **Chatbots and Virtual Assistants**: Powering conversational agents like ChatGPT.\n","\n","Challenges and Future Directions\n","\n","Despite their success, LLMs and Transformers face challenges such as high computational costs, the risk of generating biased or incorrect information, and difficulties in interpreting their decision-making processes. Researchers are exploring approaches to address these issues and enhance the efficiency and fairness of these models.\n","=======================================================================\n"]}]},{"cell_type":"code","source":["pdf_splits = split_text(pdf_full_text)\n","txt_splits = split_text(txt_full_text)\n","docx_splits = split_text(docx_full_text)"],"metadata":{"id":"Yca2q2pn2VvZ","executionInfo":{"status":"ok","timestamp":1737145881949,"user_tz":300,"elapsed":315,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["print(f\"PDF Splits: {len(pdf_splits)}\")\n","print(\"=======================================================================\")\n","print(f\"TXT Splits: {len(txt_splits)}\")\n","print(\"=======================================================================\")\n","print(f\"DOCX Splits: {len(docx_splits)}\")\n","print(\"=======================================================================\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKrncy499pnw","executionInfo":{"status":"ok","timestamp":1737145916872,"user_tz":300,"elapsed":320,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"78ede2e3-bbef-433f-8d20-137a90f96d0b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["PDF Splits: 3\n","=======================================================================\n","TXT Splits: 3\n","=======================================================================\n","DOCX Splits: 3\n","=======================================================================\n"]}]},{"cell_type":"code","source":["pdf_final_result = translate_texts_multithreaded_and_join(pdf_splits, \"es\")\n","txt_final_result = translate_texts_multithreaded_and_join(txt_splits, \"fr\")\n","docx_final_result = translate_texts_multithreaded_and_join(docx_splits, \"de\")"],"metadata":{"id":"yg9R_6lS9ySi","executionInfo":{"status":"ok","timestamp":1737145999299,"user_tz":300,"elapsed":6835,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["print(f\"PDF Result: {pdf_final_result}\")\n","print(\"=======================================================================\")\n","print(f\"TXT Result: {txt_final_result}\")\n","print(\"=======================================================================\")\n","print(f\"DOCX Result: {docx_final_result}\")\n","print(\"=======================================================================\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMniqxko-FnB","executionInfo":{"status":"ok","timestamp":1737146081542,"user_tz":300,"elapsed":292,"user":{"displayName":"Aaron Sosa","userId":"09390934792605274189"}},"outputId":"a6b169f0-fbfc-41d5-92d1-768c59c4ca2e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["PDF Result: LLMs y Transformers\n","\n","Introducción\n","Los Modelos de Lenguaje Grandes (LLMs) y los Transformers han revolucionado el campo de la inteligencia artificial, específicamente en el procesamiento del lenguaje natural (PLN). Estos modelos están diseñados para comprender, generar y manipular el lenguaje humano con una precisión notable, lo que permite una amplia gama de aplicaciones.\n","\n","Transformers\n","La arquitectura Transformer, introducida en el influyente artículo 'Attention Is All You Need' por Vaswani et al. en 2017, es la columna vertebral de muchos sistemas modernos de PLN. Aprovecha un mecanismo llamado autoatención, que permite al modelo ponderar la importancia de diferentes palabras en una oración en relación entre sí, independientemente de su distancia. Esta capacidad permite a los Transformers capturar dependencias de largo alcance en el texto de manera más eficaz que los modelos anteriores.\n","\n","Componentes Clave de los Transformers\n","- **Codificador y Decodificador**: El Transformer consta de un codificador y un decodificador. El codificador procesa la secuencia de entrada, mientras que el decodificador genera- **Mecanismo de Autoatención**: Este mecanismo ayuda al modelo a centrarse en las partes relevantes de la entrada durante el procesamiento.\n","- **Codificación Posicional**: Dado que los Transformers carecen de información secuencial inherente, se añaden codificaciones posicionales a las incrustaciones de entrada.\n","\n","Modelos de Lenguaje de Gran Escala (LLMs)\n","Los LLMs se construyen sobre la arquitectura Transformer y se entrenan con conjuntos de datos masivos para realizar una variedad de tareas lingüísticas. Algunos ejemplos de estos modelos son la serie GPT de OpenAI, BERT de Google y LLaMA de Meta. Estos modelos logran un rendimiento de vanguardia en tareas como la generación de texto, el resumen, la traducción y el cuestionamiento.\n","\n","Aplicaciones de los LLMs\n","- **Generación de Texto**: Generación de texto coherente y contextualmente apropiado.\n","- **Traducción Automática**: Traducción de texto entre idiomas con alta precisión.\n","- **Resumen**: Condensación de textos largos en resúmenes concisos.\n","- **Chatbots y Asistentes Virtuales**: Impulsar agentes conversacionales como ChatGPT.\n","Desafíos y Direcciones Futuras\n","A pesar de su éxito, los LLM y los Transformers enfrentan desafíos como los altos costos computacionales, el riesgo de generar información sesgada o incorrecta y las dificultades para interpretar sus procesos de toma de decisiones. Los investigadores están explorando enfoques para abordar estos problemas y mejorar la eficiencia y la equidad de estos modelos.\n","=======================================================================\n","TXT Result: LLM et Transformeurs\n","\n","Introduction\n","Les grands modèles de langage (LLM) et les transformeurs ont révolutionné le domaine de l'intelligence artificielle, en particulier dans le traitement du langage naturel (TLN). Ces modèles sont conçus pour comprendre, générer et manipuler le langage humain avec une précision remarquable, permettant un large éventail d'applications.\n","\n","Transformeurs\n","L'architecture Transformer, introduite dans l'article fondateur \"Attention Is All You Need\" de Vaswani et al. en 2017, est la base de nombreux systèmes de TLN modernes. Elle s'appuie sur un mécanisme appelé auto-attention, qui permet au modèle de pondérer l'importance des différents mots d'une phrase les uns par rapport aux autres, quelle que soit leur distance. Cette capacité permet aux transformeurs de saisir les dépendances à longue portée dans le texte plus efficacement que les modèles précédents.\n","\n","Composants clés des transformeurs - **Encodeur et décodeur** : Le transformeur est constitué d'un encodeur et d'un décodeur. L'encodeur traite la séquence d'entrée, tandis que le décodeur génèreproduit la séquence de sortie. - **Mécanisme d'Auto-Attention** : Ce mécanisme aide le modèle à se concentrer sur les parties pertinentes de l'entrée pendant le traitement. - **Encodage Positionnel** : Étant donné que les Transformers manquent d'informations séquentielles intrinsèques, des encodages positionnels sont ajoutés aux embeddings d'entrée.\n","\n","Grands Modèles de Langage (LLM)\n","Les LLM sont construits sur l'architecture Transformer et entraînés sur des ensembles de données massifs pour effectuer une variété de tâches linguistiques. Parmi ces modèles, on trouve la série GPT d'OpenAI, BERT de Google et LLaMA de Meta. Ces modèles atteignent des performances de pointe dans des tâches telles que la génération de texte, la synthèse, la traduction et la réponse aux questions.\n","\n","Applications des LLM - **Génération de Texte** : Générer du texte cohérent et contextuellement approprié. - **Traduction Automatique** : Traduire du texte entre les langues avec une grande précision. - **Synthèse** : Condenser de longs textes en résumés concis. - **Chatbots et Assistants Virtuels** : Alimenter des agents conversationnels comme ChatGPT.\n","ChaDéfis et orientations futures\n","\n","Malgré leur succès, les LLM et les Transformers sont confrontés à des défis tels que des coûts de calcul élevés, le risque de générer des informations biaisées ou incorrectes, et des difficultés à interpréter leurs processus de prise de décision. Les chercheurs explorent des approches pour résoudre ces problèmes et améliorer l'efficacité et l'équité de ces modèles.\n","=======================================================================\n","DOCX Result: LLMs und Transformer\n","\n","Einleitung\n","\n","Large Language Models (LLMs) und Transformer haben den Bereich der künstlichen Intelligenz, insbesondere in der Verarbeitung natürlicher Sprache (Natural Language Processing, NLP), revolutioniert. Diese Modelle sind darauf ausgelegt, menschliche Sprache mit bemerkenswerter Genauigkeit zu verstehen, zu generieren und zu manipulieren, was eine breite Palette von Anwendungen ermöglicht.\n","\n","Transformer\n","\n","Die Transformer-Architektur, die 2017 in dem bahnbrechenden Paper \"Attention Is All You Need\" von Vaswani et al. vorgestellt wurde, ist das Rückgrat vieler moderner NLP-Systeme. Sie nutzt einen Mechanismus namens Selbstaufmerksamkeit (Self-Attention), der es dem Modell ermöglicht, die Bedeutung verschiedener Wörter in einem Satz relativ zueinander zu gewichten, unabhängig von ihrer Distanz. Diese Fähigkeit ermöglicht es Transformatoren, langfristige Abhängigkeiten im Text effektiver zu erfassen als frühere Modelle.\n","\n","Schlüsselkomponenten von Transformatoren\n","\n","- **Encoder und Decoder**: Der Transformer besteht aus einem Encoder und einem Decoder. Der Encoder verarbeitet die Eingabesequenz, während der Decoder die Ausgabesequenz generiert.Ausgabesequenz.\n","- **Self-Attention-Mechanismus**: Dieser Mechanismus hilft dem Modell, sich während der Verarbeitung auf relevante Teile der Eingabe zu konzentrieren.\n","- **Positionskodierung**: Da Transformer keine inhärenten sequenziellen Informationen besitzen, werden den Eingabe-Einbettungen Positionskodierungen hinzugefügt.\n","\n","Große Sprachmodelle (LLMs)\n","\n","LLMs basieren auf der Transformer-Architektur und werden anhand von riesigen Datensätzen trainiert, um eine Vielzahl von Sprachaufgaben zu erfüllen. Beispiele für solche Modelle sind die GPT-Serie von OpenAI, BERT von Google und LLaMA von Meta. Diese Modelle erzielen eine hochmoderne Leistung bei Aufgaben wie Textgenerierung, Zusammenfassung, Übersetzung und Fragebeantwortung.\n","\n","Anwendungen von LLMs\n","\n","- **Textgenerierung**: Erzeugung von kohärentem und kontextuell angemessenem Text.\n","- **Maschinelle Übersetzung**: Übersetzung von Texten zwischen Sprachen mit hoher Genauigkeit.\n","- **Zusammenfassung**: Verdichtung langer Texte in prägnante Zusammenfassungen.\n","- **Chatbots und virtuelle Assistenten**: Unterstützung von Konversationsagenten wie ChatGPT.\n","\n","Herausforderungen undZukünftige Entwicklungen\n","\n","Trotz ihres Erfolgs stehen LLMs und Transformer vor Herausforderungen wie hohen Rechenkosten, dem Risiko der Generierung voreingenommener oder falscher Informationen und Schwierigkeiten bei der Interpretation ihrer Entscheidungsprozesse. Forschende untersuchen Ansätze, um diese Probleme zu beheben und die Effizienz und Fairness dieser Modelle zu verbessern.\n","=======================================================================\n"]}]}]}